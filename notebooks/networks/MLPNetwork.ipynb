{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test cell\"\"\"\n",
    "# Dimensions\n",
    "data_points = 100\n",
    "\n",
    "# Data\n",
    "\n",
    "xx = Variable(torch.randn(8 ,50)) # #n_im x dim\n",
    "xx_dim1 = xx.shape[0]\n",
    "D_out = xx.shape[1]\n",
    "D_in =D_out * xx_dim1\n",
    "\n",
    "xx_flat = xx.view(D_in) \n",
    "yy = torch.sum(xx, 0) #\n",
    "\n",
    "# Model parameters\n",
    "hidden_s,hidden_n = 500, 3\n",
    "\n",
    "epochs = 100\n",
    "learning_rate = 1e-5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, D_in, D_out, layer_size,hidden_n):\n",
    "        super(Net, self).__init__()  \n",
    "        \n",
    "        self.hidden_layers = hidden_n - 1\n",
    "        \n",
    "        self.first_layer = torch.nn.Linear(D_in,hidden_s)\n",
    "        self.middle_layers = nn.ModuleList();\n",
    "        for i in range(self.hidden_layers):\n",
    "            self.middle_layers.append(torch.nn.Linear(hidden_s, hidden_s))\n",
    "        self.final_layer = torch.nn.Linear(hidden_s, D_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_relu = self.first_layer(x).clamp(min=0)\n",
    "        for i in range(self.hidden_layers):\n",
    "            h_relu = self.middle_layers[i](h_relu).clamp(min=0)   \n",
    "        y_pred = self.final_layer(h_relu)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398.307373046875\n",
      "394.7516174316406\n",
      "420.50030517578125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ed2760e7fc4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torch/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torch/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "hidden_s,hidden_n = 500, 3\n",
    "data_points = 100\n",
    "\n",
    "epochs = 100\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# Get first datapoints\n",
    "xx = Variable(torch.randn(8 ,50)) # get next x\n",
    "D_out = xx.shape[1]\n",
    "D_in = D_out * xx.shape[0]\n",
    "\n",
    "xx_flat = xx.view(D_in)\n",
    "yy = torch.sum(xx, 0) # get next y\n",
    "\n",
    "# xx_flat.cuda()\n",
    "# yy.cuda()\n",
    "\n",
    "#Initialize the network and loss\n",
    "model = Net(D_in, D_out, hidden_s,hidden_n)\n",
    "# model.cuda()\n",
    "\n",
    "criterion = nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "\n",
    "#Train\n",
    "for i in range(epochs):\n",
    "    for j in range(data_points):\n",
    "        y_pred = model(xx_flat)\n",
    "        loss = criterion(y_pred, yy)\n",
    "        if i%10 is 0 and (j is data_points - 1):\n",
    "            print(loss.data[0])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        xx = Variable(torch.randn(8 ,50)) # get next x\n",
    "        xx_flat = xx.view(D_in)\n",
    "        yy = torch.sum(xx, 0) # get next y\n",
    "#         xx_flat.cuda()\n",
    "#         yy.cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Net(\n",
       "  (first_layer): Linear(in_features=400, out_features=500)\n",
       "  (middle_layers): ModuleList(\n",
       "    (0): Linear(in_features=500, out_features=500)\n",
       "    (1): Linear(in_features=500, out_features=500)\n",
       "  )\n",
       "  (final_layer): Linear(in_features=500, out_features=50)\n",
       ")>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
